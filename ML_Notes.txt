06.One hot encoding technique
dummy variable trap - 1 var can b derived from rest vari- muli co variable
u have to drop 1 dummy varible column out of all(any column)
sklearn linear regression model is aware about dummy variable trap and it drops automatically
sklearn - one hot encoder


08.Logistic Regression
Linear-predicted value is continuous
Logistic-predicted value is categorical(eg: yes/no)
eg -email spam or not -y/n
    customer will buy insurance or not y/n
    which party person will vote - D,R,I
here predicted value is categorical(eg: yes/no)
#such problems are called classification probs
Logistic reg - tech used for classificaton
Classificaion
Binary classificaton- y/n (eg: email spam or not)
Multiclass classification- which party person will vote - D,R,I


09.Decision Tree
entropy-randomness
high info gain to be choosen for selecting columns/features
gini impurity-A higher Gini index indicates greater inequality
ML algos only work onnumbers, they cannot predict text.
So every time we need to convert the columns into integer forms(cols which are having text)


10.Support Vector Machine(SVM)
Choose classification line having longer margine(distance b/w data points and line)
nearby datapoints around the line called support vecotrs hence support vector machine
hyperplane-plane in n-dim that seperates/classifies diff groups.
Gamma and Regularization(C)-
high gamma- only the close data points contribute to be support vector for the decision boundary
low gamma-the far away data points are also considered to be support vector for the decision boundary
High Regularization-No errors,overfitted
Low Regularization-Might have some errors
kernal-creating transformation on existing features so that you can draw decision boundary


11.Random Forest Algorithm
forest-tree-decision tree in ml
ensemble-when we use multiple algo to build the output(here we are taking multiple decision trees)


12-K fold Cross Validation
this technique allows you to decide which algo to use for a specific problem
option1:
use all data for training- 100% data used for training and then use it to test as well
its not the best way to measure score because the model has already seen(trained) the data and when asekd to predict will definately predict accurately
option2:
we split data into training and testing data-eg: 20% testing and 80%training-used in supervised learning, this tech works okay but still not great and perfect,eg:the 80% training data was related to algebra and the 20% on which we are testing is calculas then in this case it might not predict well atall.
option3:
k-fold Cross Validation
We divide samples into folds and do iterations and each iteration has diff folds for training and testing.
for eg: 100 samples divided into 5 folds -1,2,3,4,5
iteration 1: 1 fold-test and 2,3,4,5-train - we note down score
iteration 2: 2 fold-test and 1,3,4,5-train - we note down score
.
.
iteration 5: 5 fold-test and 1,2,3,4-train - we note down score
take average of all scores
In train_test_split, the data samples are picked randomly for training everytime you run the model and you will probably see difference in the score each time you run. So you can't say that its perfect at one time, because you will get diff results everytime.
StratifiedKFold-a little better than k-fold, it will divide the folds uniformly
eg: out of 3 fold 2 fold might be having same category and 1 fold have another and this is used for testing and other two folds for training, then model wont predict accurately.


13. K Means Clustering Algorithm
Supervised, unsupervised and reinforcement learning
Supervised-in given dataset you have class label or target variable present.
unsupervised-you have set of features, you don't know about class label or target variable.
k-popular clustering algo-in graph u have no idea about data, you try to identify clusters in data points
SSE-sum of square error (centroid se each data pont k bich ka dist(error) ka avg
elbow plot-plot of all the SSE abotained from having diff k values


14.Naive Bayes
based on bayes theorm-application: email spam detection,weather ,face detection, article categorization
Naive Bayes has three classifiers
Bernoulli-when ur features are in binary form 0 or 1(not the target variables)
multinomial-when u have discrete data (eg: movie rating from 1 to 5)
gaussian-normal distribution or bell curve


16.HyperparameterTunning
the process of choosing optimal parameter is called hypertunning.
suppose in svm we have gamma,C,kernal(parameters)

17.L1 and L2 Regularization
Overfitting-very common issue in machine learning
when you try to make your model too accurate, train it hard and try to fit too much, then in this case you will have issues with testing dataset, when it tries to predict ne w data point it might not do well because of overfitting. So you always have to keep a balance between these two extreme cases(overfitting and underfitting)
L1 and l2 - techniques to address overfitting
reduce overfitting-
l1 regularization-taking absolute value of theta along with lambda
l2 regularization-taking squared value of theta along with lambda


18.KNearestNeighborsClassification
you find out the nearest datapoints of the given data point to figureout which cluster it belongs to. you need to provide k=3 (for number of nearest neighbor) You need to carefully choose value of k, not very high not very low.

19.Principal component Analysis
used for reducing dimensions
you generally separate out dependent and independent columns. 
But incase if you have 2000,10000 columns. how will u separate.
In digits dataset of sklearn, each digit is 8x8 matrix, i.e. 64 features.
FRom these feaatures not all are important. for eg: if you consider last column having 8 features are all dark for all no. and hence not important features.We need to get rid of these unimportant features beacause:
it leads to faster training and referencing
data visualization becomes easier.
PCA-process of figuring out most important features or principal components that has the most impact on the target variables.
PCA(n_components=6)-calculates most important 6 features
Before doing PCA- you need to scale featues and accuracy might drop
























